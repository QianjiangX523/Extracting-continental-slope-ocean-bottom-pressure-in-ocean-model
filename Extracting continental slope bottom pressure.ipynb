{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35566d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import splprep, splev\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.interpolate import RegularGridInterpolator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e9a31",
   "metadata": {},
   "source": [
    "# 1. Isobath tracing and construction of an isobath stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440812ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contour-following algorithm\n",
    "\n",
    "\n",
    "def followcont1(h, hcont, istart, jstart, xcyc=None, ycyc=None, ijstart=None, exact=None):\n",
    "    \"\"\"\n",
    "    Follow a contour in a 2D array.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    h : 2D numpy array\n",
    "        Input array (real, double, or integer)\n",
    "    hcont : float\n",
    "        The contour value to follow\n",
    "    istart : int\n",
    "        i index to start looking from\n",
    "    jstart : int\n",
    "        j index to start looking from\n",
    "    xcyc : bool, optional\n",
    "        Treat the i direction as cyclic\n",
    "    ycyc : bool, optional\n",
    "        Treat the j direction as cyclic\n",
    "    exact : bool, optional\n",
    "        If True, the supplied istart,jstart must be exactly correct\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    is : numpy array\n",
    "        Fractional i indices along the contour\n",
    "    js : numpy array\n",
    "        Fractional j indices along the contour\n",
    "    ijstart : tuple, optional\n",
    "        The i and j indices of the cell where the contour starts\n",
    "    \"\"\"\n",
    "    \n",
    "    lencont1 = 1000\n",
    "    lencont = lencont1\n",
    "    nan = np.nan\n",
    "    \n",
    "    # Initialize output arrays\n",
    "    is_arr = np.full(lencont, nan, dtype=np.float64)\n",
    "    js_arr = np.full(lencont, nan, dtype=np.float64)\n",
    "    \n",
    "    s = h.shape\n",
    "    nx, ny = s[0], s[1]\n",
    "    if len(s) != 2:\n",
    "        print(f'followcont: input array should be 2D, actual dimensions = {len(s)}')\n",
    "        return None, None\n",
    "    \n",
    "    # Calculate products for contour detection\n",
    "    dxprod = (h - hcont) * (np.roll(h, -1, axis=0) - hcont)\n",
    "    dyprod = (h - hcont) * (np.roll(h, -1, axis=1) - hcont)\n",
    "    \n",
    "    if xcyc is None:\n",
    "        dxprod[-1, :] = 1.0\n",
    "    if ycyc is None:\n",
    "        dyprod[:, -1] = 1.0\n",
    "    \n",
    "    jcont = np.where((dxprod <= 0) | (dyprod <= 0))\n",
    "    if len(jcont[0]) == 0 or (dxprod[istart, jstart] > 0 and dyprod[istart, jstart] > 0 and exact is not None):\n",
    "        print(f'followcont: no contour found with value {hcont}')\n",
    "        return None, None\n",
    "    \n",
    "    ijstart_indices = np.array(jcont)\n",
    "    dist = np.sqrt((istart - ijstart_indices[0, :])**2 + (jstart - ijstart_indices[1, :])**2)\n",
    "    jmin = np.argmin(dist)\n",
    "    inow, jnow = ijstart_indices[0, jmin], ijstart_indices[1, jmin]\n",
    "    ijstart = np.array([inow, jnow])\n",
    "    \n",
    "    ndir = 0\n",
    "    iadds = np.array([+1, +1, -1, -1])\n",
    "    jadds = np.array([-1, +1, +1, -1])\n",
    "    \n",
    "    if dxprod[inow, jnow] <= 0:\n",
    "        js_arr[0] = jnow\n",
    "        is_arr[0] = inow + (h[inow, jnow] - hcont) / (h[inow, jnow] - h[inow+1, jnow])\n",
    "        ndir = 1\n",
    "        ibl = inow\n",
    "        jbl = jnow\n",
    "        if h[inow, jnow] > hcont:\n",
    "            ndir = 3\n",
    "            ibl = inow + 1\n",
    "            jbl = jnow\n",
    "    else:\n",
    "        js_arr[0] = jnow + (h[inow, jnow] - hcont) / (h[inow, jnow] - h[inow, jnow+1])\n",
    "        is_arr[0] = inow\n",
    "        ndir = 2\n",
    "        ibl = inow\n",
    "        jbl = jnow\n",
    "        if h[inow, jnow] > hcont:\n",
    "            ndir = 0\n",
    "            ibl = inow\n",
    "            jbl = jnow + 1\n",
    "    \n",
    "    nend = 0\n",
    "    nback = 0\n",
    "    nedges = 0\n",
    "    ndir0 = ndir\n",
    "    \n",
    "    while True:\n",
    "        iadd = iadds[ndir]\n",
    "        jadd = jadds[ndir]\n",
    "        inext = ibl + iadd\n",
    "        jnext = jbl + jadd\n",
    "        \n",
    "        if inext >= nx:\n",
    "            if xcyc is not None:\n",
    "                inext = 0\n",
    "            else:\n",
    "                nend = 1\n",
    "        elif inext < 0:\n",
    "            if xcyc is not None:\n",
    "                inext = nx - 1\n",
    "            else:\n",
    "                nend = 1\n",
    "        \n",
    "        if jnext >= ny:\n",
    "            if ycyc is not None:\n",
    "                jnext = 0\n",
    "            else:\n",
    "                nend = 1\n",
    "        elif jnext < 0:\n",
    "            if ycyc is not None:\n",
    "                jnext = ny - 1\n",
    "            else:\n",
    "                nend = 1\n",
    "        \n",
    "        itr = inext\n",
    "        jtr = jnext\n",
    "        ibr = ibl + (ndir % 2) * (itr - ibl)\n",
    "        jbr = jtr + (ndir % 2) * (jbl - jtr)\n",
    "        itl = itr + (ndir % 2) * (ibl - itr)\n",
    "        jtl = jbl + (ndir % 2) * (jtr - jbl)\n",
    "        \n",
    "        hbl = h[ibl, jbl]\n",
    "        hbr = h[ibr, jbr]\n",
    "        \n",
    "        if hbl != hbr:\n",
    "            frac = (hcont - hbl) / (hbr - hbl)\n",
    "        else:\n",
    "            frac = 0.5\n",
    "        \n",
    "        frac = min(frac, 1)\n",
    "        frac = max(frac, 0)\n",
    "        \n",
    "        di = ibr - ibl\n",
    "        if di > nx / 2:\n",
    "            di -= nx\n",
    "        if di < -nx / 2:\n",
    "            di += nx\n",
    "            \n",
    "        dj = jbr - jbl\n",
    "        if dj > ny / 2:\n",
    "            dj -= ny\n",
    "        if dj < -ny / 2:\n",
    "            dj += ny\n",
    "            \n",
    "        inew = ibl + di * frac\n",
    "        jnew = jbl + dj * frac\n",
    "        \n",
    "        nedges += 1\n",
    "        if nedges > lencont:\n",
    "            is_arr = np.concatenate([is_arr, np.full(lencont1, nan)])\n",
    "            js_arr = np.concatenate([js_arr, np.full(lencont1, nan)])\n",
    "            lencont += lencont1\n",
    "        \n",
    "        is_arr[nedges-1] = inew\n",
    "        js_arr[nedges-1] = jnew\n",
    "        \n",
    "        if nend == 0:\n",
    "            if h[itl, jtl] > hcont:\n",
    "                ndir = (ndir + 1) % 4\n",
    "            elif h[itr, jtr] > hcont:\n",
    "                ibl = itl\n",
    "                jbl = jtl\n",
    "            elif h[ibr, jbr] > hcont:\n",
    "                ndir = (ndir + 3) % 4\n",
    "                ibl = itr\n",
    "                jbl = jtr\n",
    "            else:\n",
    "                nend = 1\n",
    "            \n",
    "            if nedges != 1 and is_arr[0] == is_arr[nedges-1] and js_arr[0] == js_arr[nedges-1]:\n",
    "                nback = 1\n",
    "        \n",
    "        if nend == 1 or nback == 1:\n",
    "            break\n",
    "    \n",
    "    finite_mask = np.isfinite(is_arr)\n",
    "    is_arr = is_arr[finite_mask]\n",
    "    js_arr = js_arr[finite_mask]\n",
    "    \n",
    "    if nback != 1:\n",
    "        ndir = (ndir0 + 2) % 4\n",
    "        if ndir0 == 0:\n",
    "            ibl = np.floor(is_arr[0]).astype(int)\n",
    "            jbl = np.floor(js_arr[0]).astype(int)\n",
    "        elif ndir0 == 1:\n",
    "            ibl = np.ceil(is_arr[0]).astype(int)\n",
    "            jbl = np.floor(js_arr[0]).astype(int)\n",
    "        elif ndir0 == 2:\n",
    "            ibl = np.floor(is_arr[0]).astype(int)\n",
    "            jbl = np.ceil(js_arr[0]).astype(int)\n",
    "        else:\n",
    "            ibl = np.floor(is_arr[0]).astype(int)\n",
    "            jbl = np.floor(js_arr[0]).astype(int)\n",
    "        \n",
    "        nend = 0\n",
    "        nedges = 0\n",
    "        isb = np.full(lencont1, nan)\n",
    "        jsb = np.full(lencont1, nan)\n",
    "        lencont = lencont1\n",
    "        \n",
    "        while True:\n",
    "            iadd = iadds[ndir]\n",
    "            jadd = jadds[ndir]\n",
    "            inext = ibl + iadd\n",
    "            jnext = jbl + jadd\n",
    "            insav = inext\n",
    "            jnsav = jnext\n",
    "            \n",
    "            if inext >= nx:\n",
    "                if xcyc is not None:\n",
    "                    inext = 0\n",
    "                else:\n",
    "                    nend = 1\n",
    "            elif inext < 0:\n",
    "                if xcyc is not None:\n",
    "                    inext = nx - 1\n",
    "                else:\n",
    "                    nend = 1\n",
    "            \n",
    "            if jnext >= ny:\n",
    "                if ycyc is not None:\n",
    "                    jnext = 0\n",
    "                else:\n",
    "                    nend = 1\n",
    "            elif jnext < 0:\n",
    "                if ycyc is not None:\n",
    "                    jnext = ny - 1\n",
    "                else:\n",
    "                    nend = 1\n",
    "            \n",
    "            itr = inext\n",
    "            jtr = jnext\n",
    "            ibr = ibl + (ndir % 2) * (itr - ibl)\n",
    "            jbr = jtr + (ndir % 2) * (jbl - jtr)\n",
    "            itl = itr + (ndir % 2) * (ibl - itr)\n",
    "            jtl = jbl + (ndir % 2) * (jtr - jbl)\n",
    "            \n",
    "            hbl = -h[ibl, jbl]\n",
    "            hbr = -h[ibr, jbr]\n",
    "            \n",
    "            if hbl != hbr:\n",
    "                frac = (-hcont - hbl) / (hbr - hbl)\n",
    "            else:\n",
    "                frac = 0.5\n",
    "            \n",
    "            frac = min(frac, 1)\n",
    "            frac = max(frac, 0)\n",
    "            \n",
    "            di = ibr - ibl\n",
    "            if di > nx / 2:\n",
    "                di -= nx\n",
    "            if di < -nx / 2:\n",
    "                di += nx\n",
    "                \n",
    "            dj = jbr - jbl\n",
    "            if dj > ny / 2:\n",
    "                dj -= ny\n",
    "            if dj < -ny / 2:\n",
    "                dj += ny\n",
    "                \n",
    "            inew = ibl + di * frac\n",
    "            jnew = jbl + dj * frac\n",
    "            \n",
    "            nedges += 1\n",
    "            if nedges > lencont:\n",
    "                isb = np.concatenate([isb, np.full(lencont1, nan)])\n",
    "                jsb = np.concatenate([jsb, np.full(lencont1, nan)])\n",
    "                lencont += lencont1\n",
    "            \n",
    "            isb[nedges-1] = inew\n",
    "            jsb[nedges-1] = jnew\n",
    "            \n",
    "            if nend == 0:\n",
    "                if h[itl, jtl] <= hcont:\n",
    "                    ndir = (ndir + 1) % 4\n",
    "                elif h[itr, jtr] <= hcont:\n",
    "                    ibl = itl\n",
    "                    jbl = jtl\n",
    "                elif h[ibr, jbr] <= hcont:\n",
    "                    ndir = (ndir + 3) % 4\n",
    "                    ibl = itr\n",
    "                    jbl = jtr\n",
    "                else:\n",
    "                    nend = 1\n",
    "            \n",
    "            if nend == 1:\n",
    "                break\n",
    "        \n",
    "        finite_mask = np.isfinite(isb)\n",
    "        isb = isb[finite_mask]\n",
    "        jsb = jsb[finite_mask]\n",
    "        \n",
    "        if len(isb) > 1:\n",
    "            is_arr = np.concatenate([np.flip(isb[1:]), is_arr])\n",
    "            js_arr = np.concatenate([np.flip(jsb[1:]), js_arr])\n",
    "    \n",
    "    if exact is not None and is_arr[-1] == istart and js_arr[-1] == jstart:\n",
    "        is_arr = np.flip(is_arr)\n",
    "        js_arr = np.flip(js_arr)\n",
    "    \n",
    "    if ijstart is not None:\n",
    "        return is_arr, js_arr, ijstart\n",
    "    else:\n",
    "        return is_arr, js_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec1acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate contour stack\n",
    "\n",
    "\n",
    "def generate_contour_stack(h, start=1000, end=3000, interval=1, istart=None, jstart=None, xcyc=None, ycyc=None):\n",
    "    \"\"\"\n",
    "    Generate a stack of contours for a range of contour values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    h : 2D numpy array\n",
    "        Input array\n",
    "    start : float\n",
    "        Starting contour value\n",
    "    end : float\n",
    "        Ending contour value\n",
    "    interval : float\n",
    "        Step between contour values\n",
    "    istart : int, optional\n",
    "        Starting i index for contour following\n",
    "    jstart : int, optional\n",
    "        Starting j index for contour following\n",
    "    xcyc : bool, optional\n",
    "        Treat i direction as cyclic\n",
    "    ycyc : bool, optional\n",
    "        Treat j direction as cyclic\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    contour_stack : list of tuples\n",
    "        Each tuple contains (contour_value, is_coords, js_coords)\n",
    "    \"\"\"\n",
    "    \n",
    "    if istart is None:\n",
    "        istart = h.shape[0] // 2  # Default to center if not specified\n",
    "    if jstart is None:\n",
    "        jstart = h.shape[1] // 2  # Default to center if not specified\n",
    "    \n",
    "    contour_stack = []\n",
    "    \n",
    "    for hcont in np.arange(start, end + interval, interval):\n",
    "        # Call the followcont function\n",
    "        result = followcont1(h, hcont, istart, jstart, xcyc=xcyc, ycyc=ycyc)\n",
    "        \n",
    "        if result is not None:\n",
    "            if len(result) == 2:\n",
    "                is_coords, js_coords = result\n",
    "            else:\n",
    "                is_coords, js_coords, _ = result\n",
    "            \n",
    "            # Only add to stack if we got valid coordinates\n",
    "            if len(is_coords) > 0 and len(js_coords) > 0:\n",
    "                contour_stack.append((hcont, is_coords, js_coords))\n",
    "    \n",
    "    return contour_stack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac874a",
   "metadata": {},
   "source": [
    "# 2. Identification of cross-slope points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid smoothing procedure \n",
    "# combining adaptive moving average filtering, Gaussian convolution with reflective boundaries, and spline-based curve\n",
    "# reconstruction subject to arc-length preservation.\n",
    "\n",
    "# ========== Ultra-strong contour smoothing ==========\n",
    "# ========== Improved contour smoothing (preserves length) ==========\n",
    "def ultra_smooth_contour_preserve_length(contour_points, sigma=2.0, spline_smooth=0.001, iterations=2):\n",
    "    \"\"\"\n",
    "    Improved contour smoothing that preserves the original contour length\n",
    "    \"\"\"\n",
    "    if len(contour_points) < 50:\n",
    "        return contour_points\n",
    "    \n",
    "    # Save original endpoints\n",
    "    original_start = contour_points[0].copy()\n",
    "    original_end = contour_points[-1].copy()\n",
    "    original_length = len(contour_points)\n",
    "    \n",
    "    current_points = contour_points.copy()\n",
    "    \n",
    "    # Use smaller sigma and fewer iterations\n",
    "    for iter_num in range(iterations):\n",
    "        i_coords = current_points[:, 0]\n",
    "        j_coords = current_points[:, 1]\n",
    "        \n",
    "        # Use fixed sigma to avoid progressive over-smoothing\n",
    "        current_sigma = sigma\n",
    "        i_smooth = gaussian_filter1d(i_coords, sigma=current_sigma, mode='reflect')\n",
    "        j_smooth = gaussian_filter1d(j_coords, sigma=current_sigma, mode='reflect')\n",
    "        \n",
    "        current_points = np.column_stack([i_smooth, j_smooth])\n",
    "    \n",
    "    try:\n",
    "        t = np.linspace(0, 1, len(current_points))\n",
    "        # Use a larger smoothing factor to avoid overfitting\n",
    "        tck, u = splprep(\n",
    "            [current_points[:, 0], current_points[:, 1]],\n",
    "            s=spline_smooth * len(current_points) * 10,\n",
    "            k=3\n",
    "        )\n",
    "        \n",
    "        # Preserve the original number of points\n",
    "        u_new = np.linspace(0, 1, original_length)\n",
    "        i_final, j_final = splev(u_new, tck)\n",
    "        \n",
    "        # Light additional smoothing with a smaller sigma\n",
    "        i_final = gaussian_filter1d(i_final, sigma=1.0, mode='reflect')\n",
    "        j_final = gaussian_filter1d(j_final, sigma=1.0, mode='reflect')\n",
    "        \n",
    "        smoothed_points = np.column_stack([i_final, j_final])\n",
    "        \n",
    "        # Enforce original endpoints\n",
    "        smoothed_points[0] = original_start\n",
    "        smoothed_points[-1] = original_end\n",
    "        \n",
    "        return smoothed_points\n",
    "        \n",
    "    except Exception:\n",
    "        # If spline fitting fails, return Gaussian-smoothed result with preserved endpoints\n",
    "        current_points[0] = original_start\n",
    "        current_points[-1] = original_end\n",
    "        return current_points\n",
    "\n",
    "\n",
    "def ultra_smooth_contour_stack_preserve_length(contour_stack, sigma=2.0, spline_smooth=0.001):\n",
    "    \"\"\"\n",
    "    Improved contour stack smoothing that preserves the length of each contour\n",
    "    \"\"\"\n",
    "    smoothed_stack = []\n",
    "    \n",
    "    for depth, i_coords, j_coords in contour_stack:\n",
    "        points = np.column_stack([i_coords, j_coords])\n",
    "        \n",
    "        if len(points) > 80:\n",
    "            # Use the improved smoothing method\n",
    "            smoothed_points = ultra_smooth_contour_preserve_length(points, sigma, spline_smooth)\n",
    "        elif len(points) > 30:\n",
    "            # Use Gaussian filtering with reflective boundary conditions\n",
    "            i_smooth = gaussian_filter1d(i_coords, sigma=3.0, mode='reflect')\n",
    "            j_smooth = gaussian_filter1d(j_coords, sigma=3.0, mode='reflect')\n",
    "            smoothed_points = np.column_stack([i_smooth, j_smooth])\n",
    "        else:\n",
    "            # Apply very light smoothing for short contours\n",
    "            i_smooth = gaussian_filter1d(i_coords, sigma=1.5, mode='reflect')\n",
    "            j_smooth = gaussian_filter1d(j_coords, sigma=1.5, mode='reflect')\n",
    "            smoothed_points = np.column_stack([i_smooth, j_smooth])\n",
    "        \n",
    "        smoothed_stack.append((depth, smoothed_points[:, 0], smoothed_points[:, 1]))\n",
    "    \n",
    "    return smoothed_stack\n",
    "\n",
    "\n",
    "# ========== Alternative approach: moving-average-based smoothing ==========\n",
    "def moving_average_smooth_contour(contour_points, window_size=5):\n",
    "    \"\"\"\n",
    "    Smooth contours using a moving average, better preserving contour length\n",
    "    \"\"\"\n",
    "    if len(contour_points) < window_size * 2:\n",
    "        return contour_points\n",
    "    \n",
    "    # Save original endpoints\n",
    "    original_start = contour_points[0].copy()\n",
    "    original_end = contour_points[-1].copy()\n",
    "    \n",
    "    i_coords = contour_points[:, 0]\n",
    "    j_coords = contour_points[:, 1]\n",
    "    \n",
    "    # Create moving average kernel\n",
    "    kernel = np.ones(window_size) / window_size\n",
    "    \n",
    "    # Apply moving average\n",
    "    i_smooth = np.convolve(i_coords, kernel, mode='same')\n",
    "    j_smooth = np.convolve(j_coords, kernel, mode='same')\n",
    "    \n",
    "    # Handle edge effects\n",
    "    half_window = window_size // 2\n",
    "    i_smooth[:half_window] = i_coords[:half_window]\n",
    "    i_smooth[-half_window:] = i_coords[-half_window:]\n",
    "    j_smooth[:half_window] = j_coords[:half_window]\n",
    "    j_smooth[-half_window:] = j_coords[-half_window:]\n",
    "    \n",
    "    smoothed_points = np.column_stack([i_smooth, j_smooth])\n",
    "    \n",
    "    # Preserve original endpoints\n",
    "    smoothed_points[0] = original_start\n",
    "    smoothed_points[-1] = original_end\n",
    "    \n",
    "    return smoothed_points\n",
    "\n",
    "\n",
    "def moving_average_smooth_stack(contour_stack, window_size=5):\n",
    "    \"\"\"\n",
    "    Smooth a stack of contours using a moving average\n",
    "    \"\"\"\n",
    "    smoothed_stack = []\n",
    "    \n",
    "    for depth, i_coords, j_coords in contour_stack:\n",
    "        points = np.column_stack([i_coords, j_coords])\n",
    "        \n",
    "        # Adjust window size based on contour length\n",
    "        adaptive_window = min(window_size, len(points) // 10)\n",
    "        if adaptive_window < 3:\n",
    "            adaptive_window = 3\n",
    "            \n",
    "        smoothed_points = moving_average_smooth_contour(points, adaptive_window)\n",
    "        smoothed_stack.append((depth, smoothed_points[:, 0], smoothed_points[:, 1]))\n",
    "    \n",
    "    return smoothed_stack\n",
    "\n",
    "\n",
    "# ========== Best approach: hybrid smoothing ==========\n",
    "def hybrid_smooth_contour(contour_points, sigma=1.5, window_size=5, spline_smooth=0.01):\n",
    "    \"\"\"\n",
    "    Hybrid smoothing method: moving average + light Gaussian filtering + conservative spline\n",
    "    \"\"\"\n",
    "    if len(contour_points) < 50:\n",
    "        return contour_points\n",
    "    \n",
    "    # Save original endpoints\n",
    "    original_start = contour_points[0].copy()\n",
    "    original_end = contour_points[-1].copy()\n",
    "    original_length = len(contour_points)\n",
    "    \n",
    "    # Step 1: moving average smoothing\n",
    "    points_ma = moving_average_smooth_contour(contour_points, window_size)\n",
    "    \n",
    "    # Step 2: light Gaussian filtering\n",
    "    i_coords = points_ma[:, 0]\n",
    "    j_coords = points_ma[:, 1]\n",
    "    i_gaussian = gaussian_filter1d(i_coords, sigma=sigma, mode='reflect')\n",
    "    j_gaussian = gaussian_filter1d(j_coords, sigma=sigma, mode='reflect')\n",
    "    \n",
    "    points_gaussian = np.column_stack([i_gaussian, j_gaussian])\n",
    "    \n",
    "    try:\n",
    "        # Step 3: conservative spline interpolation\n",
    "        t = np.linspace(0, 1, len(points_gaussian))\n",
    "        tck, u = splprep(\n",
    "            [points_gaussian[:, 0], points_gaussian[:, 1]],\n",
    "            s=spline_smooth * len(points_gaussian) * 5,\n",
    "            k=3\n",
    "        )\n",
    "        \n",
    "        # Preserve the original number of points\n",
    "        u_new = np.linspace(0, 1, original_length)\n",
    "        i_final, j_final = splev(u_new, tck)\n",
    "        \n",
    "        final_points = np.column_stack([i_final, j_final])\n",
    "        \n",
    "        # Enforce original endpoints\n",
    "        final_points[0] = original_start\n",
    "        final_points[-1] = original_end\n",
    "        \n",
    "        return final_points\n",
    "        \n",
    "    except Exception:\n",
    "        # If spline fitting fails, return Gaussian-smoothed result\n",
    "        points_gaussian[0] = original_start\n",
    "        points_gaussian[-1] = original_end\n",
    "        return points_gaussian\n",
    "\n",
    "\n",
    "def hybrid_smooth_contour_stack(contour_stack, sigma=1.5, window_size=5, spline_smooth=0.01):\n",
    "    \"\"\"\n",
    "    Apply hybrid smoothing to a stack of contours\n",
    "    \"\"\"\n",
    "    smoothed_stack = []\n",
    "    \n",
    "    for depth, i_coords, j_coords in contour_stack:\n",
    "        points = np.column_stack([i_coords, j_coords])\n",
    "        \n",
    "        if len(points) > 60:\n",
    "            smoothed_points = hybrid_smooth_contour(points, sigma, window_size, spline_smooth)\n",
    "        else:\n",
    "            # Use moving average for short contours\n",
    "            smoothed_points = moving_average_smooth_contour(points, min(5, len(points) // 10))\n",
    "        \n",
    "        smoothed_stack.append((depth, smoothed_points[:, 0], smoothed_points[:, 1]))\n",
    "    \n",
    "    return smoothed_stack\n",
    "\n",
    "\n",
    "# ========== Apply ultra-strong smoothing ==========\n",
    "smoothed_selected_contours = hybrid_smooth_contour_stack(\n",
    "    selected_contours, sigma=25.0, spline_smooth=0.0005\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d762ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification of cross-slope points on the smoothed contours\n",
    "\n",
    "# ========== Large-scale elliptical neighborhood normal averaging ==========\n",
    "def calculate_elliptical_normal_large(contour_points, point_index, \n",
    "                                      ellipse_ratio=0.15,\n",
    "                                      num_neighbors=40,\n",
    "                                      max_search_ratio=0.4):\n",
    "    if len(contour_points) < 15:\n",
    "        return calculate_normal_at_point_basic(contour_points, point_index)\n",
    "    \n",
    "    # Initial local normal\n",
    "    initial_normal = calculate_normal_at_point_basic(contour_points, point_index)\n",
    "    if np.linalg.norm(initial_normal) < 0.1:\n",
    "        return initial_normal\n",
    "    \n",
    "    current_point = contour_points[point_index]\n",
    "    contour_length = len(contour_points)\n",
    "    \n",
    "    # Vectors and distances from the reference point\n",
    "    vectors = contour_points - current_point\n",
    "    distances = np.linalg.norm(vectors, axis=1)\n",
    "    \n",
    "    # Compute angular separation relative to the initial normal\n",
    "    angles = []\n",
    "    for vec in vectors:\n",
    "        if np.linalg.norm(vec) > 0.1:\n",
    "            vec_norm = vec / np.linalg.norm(vec)\n",
    "            angle = np.arccos(np.clip(np.dot(vec_norm, initial_normal), -1.0, 1.0))\n",
    "            angles.append(min(angle, np.pi - angle))\n",
    "        else:\n",
    "            angles.append(0)\n",
    "    \n",
    "    angles = np.array(angles)\n",
    "    \n",
    "    # Elliptical distance metric (elongated along the normal direction)\n",
    "    ellipse_distances = np.sqrt(\n",
    "        (distances * np.cos(angles))**2 / ellipse_ratio + \n",
    "        (distances * np.sin(angles))**2 * ellipse_ratio\n",
    "    )\n",
    "    \n",
    "    # Select candidate neighbors\n",
    "    max_neighbors = min(num_neighbors * 2, len(contour_points))\n",
    "    candidate_indices = np.argsort(ellipse_distances)[:max_neighbors]\n",
    "    \n",
    "    # Further restrict neighbors by distance percentile\n",
    "    max_distance = np.percentile(distances[candidate_indices], 70)\n",
    "    valid_candidates = [\n",
    "        idx for idx in candidate_indices\n",
    "        if distances[idx] <= max_distance and idx != point_index\n",
    "    ]\n",
    "    \n",
    "    # Fallback if too few valid neighbors\n",
    "    if len(valid_candidates) < 10:\n",
    "        valid_candidates = candidate_indices[:min(20, len(candidate_indices))]\n",
    "        valid_candidates = [idx for idx in valid_candidates if idx != point_index]\n",
    "    \n",
    "    # Compute weighted average of neighboring normals\n",
    "    neighbor_normals = []\n",
    "    neighbor_weights = []\n",
    "    \n",
    "    for idx in valid_candidates:\n",
    "        neighbor_normal = calculate_normal_at_point_basic(contour_points, idx)\n",
    "        \n",
    "        if np.linalg.norm(neighbor_normal) > 0.1:\n",
    "            weight = 1.0 / (1.0 + distances[idx])\n",
    "            neighbor_normals.append(neighbor_normal)\n",
    "            neighbor_weights.append(weight)\n",
    "    \n",
    "    if len(neighbor_normals) == 0:\n",
    "        return initial_normal\n",
    "    \n",
    "    neighbor_normals_array = np.array(neighbor_normals)\n",
    "    neighbor_weights_array = np.array(neighbor_weights)\n",
    "    neighbor_weights_array /= np.sum(neighbor_weights_array)\n",
    "    \n",
    "    avg_normal = np.zeros(2)\n",
    "    for i in range(len(neighbor_normals)):\n",
    "        avg_normal += neighbor_normals_array[i] * neighbor_weights_array[i]\n",
    "    \n",
    "    avg_norm = np.linalg.norm(avg_normal)\n",
    "    \n",
    "    # Normalize and ensure consistent orientation\n",
    "    if avg_norm > 0.1:\n",
    "        avg_normal /= avg_norm\n",
    "        if np.dot(avg_normal, initial_normal) < 0:\n",
    "            avg_normal = -avg_normal\n",
    "        return avg_normal\n",
    "    else:\n",
    "        return initial_normal\n",
    "\n",
    "\n",
    "# ========== Basic local normal estimation ==========\n",
    "def calculate_normal_at_point_basic(contour_points, point_index, window_size=9):\n",
    "    if len(contour_points) < 3:\n",
    "        return np.array([0, 0])\n",
    "    \n",
    "    start_idx = max(0, point_index - window_size)\n",
    "    end_idx = min(len(contour_points), point_index + window_size + 1)\n",
    "    \n",
    "    if end_idx - start_idx < 3:\n",
    "        return np.array([0, 0])\n",
    "    \n",
    "    window_points = contour_points[start_idx:end_idx]\n",
    "    t = np.arange(len(window_points))\n",
    "    \n",
    "    try:\n",
    "        # Linear fit for i-coordinate\n",
    "        A_i = np.column_stack([t, np.ones(len(t))])\n",
    "        coeff_i, _, _, _ = np.linalg.lstsq(A_i, window_points[:, 0], rcond=None)\n",
    "        \n",
    "        # Linear fit for j-coordinate\n",
    "        A_j = np.column_stack([t, np.ones(len(t))])\n",
    "        coeff_j, _, _, _ = np.linalg.lstsq(A_j, window_points[:, 1], rcond=None)\n",
    "        \n",
    "        # Tangent vector\n",
    "        tangent = np.array([coeff_i[0], coeff_j[0]])\n",
    "        tangent_norm = np.linalg.norm(tangent)\n",
    "        \n",
    "        if tangent_norm > 0:\n",
    "            tangent /= tangent_norm\n",
    "            normal = np.array([-tangent[1], tangent[0]])\n",
    "            return normal\n",
    "        else:\n",
    "            return np.array([0, 0])\n",
    "    except Exception:\n",
    "        return np.array([0, 0])\n",
    "\n",
    "\n",
    "# ========== Select reference points on each contour and compute large-scale averaged normals ==========\n",
    "def select_points_with_large_elliptical_normals(contour_stack, points_per_contour=6):\n",
    "    reference_points = []\n",
    "    reference_normals = []\n",
    "    \n",
    "    for depth_idx, (depth, i_coords, j_coords) in enumerate(contour_stack):\n",
    "        contour_points = np.column_stack([i_coords, j_coords])\n",
    "        \n",
    "        # Determine sampling indices along the contour\n",
    "        if len(contour_points) < points_per_contour * 2:\n",
    "            indices = np.arange(len(contour_points))\n",
    "        else:\n",
    "            start_idx = len(contour_points) // 55\n",
    "            end_idx = len(contour_points) - len(contour_points) // 15\n",
    "            indices = np.linspace(start_idx, end_idx - 1, points_per_contour, dtype=int)\n",
    "        \n",
    "        points_on_contour = []\n",
    "        normals_on_contour = []\n",
    "        \n",
    "        for idx in indices:\n",
    "            point = contour_points[idx]\n",
    "            normal = calculate_elliptical_normal_large(\n",
    "                contour_points, idx,\n",
    "                ellipse_ratio=0.15,\n",
    "                num_neighbors=50,\n",
    "                max_search_ratio=0.5\n",
    "            )\n",
    "            \n",
    "            if np.linalg.norm(normal) > 0.2:\n",
    "                points_on_contour.append(point)\n",
    "                normals_on_contour.append(normal)\n",
    "        \n",
    "        reference_points.append((depth, points_on_contour))\n",
    "        reference_normals.append((depth, normals_on_contour))\n",
    "    \n",
    "    return reference_points, reference_normals\n",
    "\n",
    "\n",
    "# ========== Select reference points and compute large-scale averaged normals ==========\n",
    "points_per_contour = 20 \n",
    "reference_points, reference_normals = select_points_with_large_elliptical_normals(\n",
    "    smoothed_selected_contours, points_per_contour\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65811610",
   "metadata": {},
   "source": [
    "# 3. Extraction of bottom pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c435145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Helper function: convert fractional coordinates to actual coordinates ==========\n",
    "def fractional_to_actual(lon_array, lat_array, y_frac, x_frac):\n",
    "    \"\"\"\n",
    "    Convert fractional grid coordinates to actual longitude and latitude\n",
    "    \n",
    "    Parameters:\n",
    "        lon_array: 2D longitude array (y, x)\n",
    "        lat_array: 2D latitude array (y, x)\n",
    "        y_frac: fractional y-coordinates\n",
    "        x_frac: fractional x-coordinates\n",
    "    \n",
    "    Returns:\n",
    "        lon_pts: interpolated longitude values\n",
    "        lat_pts: interpolated latitude values\n",
    "    \"\"\"\n",
    "    # Clip coordinates to valid grid range\n",
    "    y_coords = np.clip(y_frac, 0, lon_array.shape[0] - 1)\n",
    "    x_coords = np.clip(x_frac, 0, lon_array.shape[1] - 1)\n",
    "    \n",
    "    # Bilinear interpolation to obtain longitude and latitude\n",
    "    y_floor = np.floor(y_coords).astype(int)\n",
    "    y_ceil = np.ceil(y_coords).astype(int)\n",
    "    x_floor = np.floor(x_coords).astype(int)\n",
    "    x_ceil = np.ceil(x_coords).astype(int)\n",
    "    \n",
    "    # Prevent index out of bounds\n",
    "    y_ceil = np.minimum(y_ceil, lon_array.shape[0] - 1)\n",
    "    x_ceil = np.minimum(x_ceil, lon_array.shape[1] - 1)\n",
    "    \n",
    "    # Interpolation weights\n",
    "    y_weight = y_coords - y_floor\n",
    "    x_weight = x_coords - x_floor\n",
    "    \n",
    "    # Bilinear interpolation\n",
    "    lon_pts = (\n",
    "        (1 - y_weight) * (1 - x_weight) * lon_array[y_floor, x_floor]\n",
    "        + (1 - y_weight) * x_weight * lon_array[y_floor, x_ceil]\n",
    "        + y_weight * (1 - x_weight) * lon_array[y_ceil, x_floor]\n",
    "        + y_weight * x_weight * lon_array[y_ceil, x_ceil]\n",
    "    )\n",
    "    \n",
    "    lat_pts = (\n",
    "        (1 - y_weight) * (1 - x_weight) * lat_array[y_floor, x_floor]\n",
    "        + (1 - y_weight) * x_weight * lat_array[y_floor, x_ceil]\n",
    "        + y_weight * (1 - x_weight) * lat_array[y_ceil, x_floor]\n",
    "        + y_weight * x_weight * lat_array[y_ceil, x_ceil]\n",
    "    )\n",
    "    \n",
    "    return lon_pts, lat_pts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb78dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Extract OBP at reference points ==========\n",
    "def extract_pressure_at_reference_points(lon_sub, lat_sub, pbo_sub, reference_points):\n",
    "    \"\"\"\n",
    "    Extract pressure values at reference point locations\n",
    "    \n",
    "    Parameters:\n",
    "        lon_sub: 2D longitude array (y, x)\n",
    "        lat_sub: 2D latitude array (y, x)\n",
    "        pbo_sub: 3D pressure array (time, y, x)\n",
    "        reference_points: list of reference points\n",
    "                          [(depth, [point1, point2, ...]), ...]\n",
    "    \n",
    "    Returns:\n",
    "        pbo_ds: xarray Dataset\n",
    "    \"\"\"\n",
    "    # Get depth list\n",
    "    depths = [depth for depth, _ in reference_points]\n",
    "    max_points = max(len(points) for _, points in reference_points)\n",
    "    t_steps = pbo_sub.shape[0]\n",
    "    \n",
    "    # Initialize output arrays\n",
    "    pressure_data = np.full((t_steps, len(depths), max_points), np.nan)\n",
    "    lon_data = np.full((len(depths), max_points), np.nan)\n",
    "    lat_data = np.full((len(depths), max_points), np.nan)\n",
    "    \n",
    "    print(\n",
    "        f\"Start extracting OBP at reference points: \"\n",
    "        f\"{len(depths)} depth levels, up to {max_points} points per depth, \"\n",
    "        f\"{t_steps} time steps\"\n",
    "    )\n",
    "    \n",
    "    # Loop over reference points at each depth\n",
    "    for depth_idx, (depth, points) in enumerate(reference_points):\n",
    "        if len(points) == 0:\n",
    "            continue\n",
    "            \n",
    "        n_points = len(points)\n",
    "        \n",
    "        # Extract y, x coordinates\n",
    "        y_coords = np.array([point[0] for point in points])\n",
    "        x_coords = np.array([point[1] for point in points])\n",
    "        \n",
    "        # Convert to actual longitude and latitude\n",
    "        lon_pts, lat_pts = fractional_to_actual(\n",
    "            lon_sub, lat_sub, y_coords, x_coords\n",
    "        )\n",
    "        \n",
    "        # Store coordinates\n",
    "        lon_data[depth_idx, :n_points] = lon_pts\n",
    "        lat_data[depth_idx, :n_points] = lat_pts\n",
    "        \n",
    "        # Prepare interpolation points (ensure valid range)\n",
    "        points_array = np.column_stack([\n",
    "            np.clip(y_coords, 0, lon_sub.shape[0] - 1.0001),\n",
    "            np.clip(x_coords, 0, lon_sub.shape[1] - 1.0001)\n",
    "        ])\n",
    "        \n",
    "        # Interpolate pressure for each time step\n",
    "        for t in range(t_steps):\n",
    "            interp = RegularGridInterpolator(\n",
    "                (np.arange(lon_sub.shape[0]), np.arange(lon_sub.shape[1])),\n",
    "                pbo_sub[t],\n",
    "                method=\"linear\",\n",
    "                bounds_error=False,\n",
    "                fill_value=np.nan\n",
    "            )\n",
    "            pressure_data[t, depth_idx, :n_points] = interp(points_array)\n",
    "        \n",
    "        print(f\"Depth {depth} m: processed {n_points} reference points\")\n",
    "    \n",
    "    # Create xarray Dataset\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            \"pressure\": ([\"time\", \"depth\", \"point\"], pressure_data),\n",
    "            \"lon\": ([\"depth\", \"point\"], lon_data),\n",
    "            \"lat\": ([\"depth\", \"point\"], lat_data),\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": np.arange(t_steps),\n",
    "            \"depth\": depths,\n",
    "            \"point\": np.arange(max_points),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return obp_ds\n",
    "\n",
    "\n",
    "def extract_nth_point_obp(obp_ds, n=0):\n",
    "    \"\"\"\n",
    "    Extract OBP time series at the n-th point (point = n) as a (time, depth) array\n",
    "    \n",
    "    Parameters:\n",
    "        obp_ds: xarray Dataset\n",
    "        n: point index (starting from 0)\n",
    "    \"\"\"\n",
    "    # Check index validity\n",
    "    if n >= obp_ds.dims[\"point\"]:\n",
    "        print(\n",
    "            f\"Error: index {n} is out of range, \"\n",
    "            f\"maximum index is {obp_ds.dims['point'] - 1}\"\n",
    "        )\n",
    "        return None\n",
    "    \n",
    "    # Extract pressure at the n-th point\n",
    "    nth_point_obp = obp_ds.pressure.isel(point=n).values  # shape: (time, depth)\n",
    "    \n",
    "    print(f\"OBP extraction for point {n} completed!\")\n",
    "    print(f\"Array shape: {nth_point_obp.shape} (time × depth)\")\n",
    "    print(f\"Number of time steps: {nth_point_obp.shape[0]}\")\n",
    "    print(f\"Number of depth levels: {nth_point_obp.shape[1]}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    valid_mask = ~np.isnan(nth_point_obp)\n",
    "    print(f\"Valid data points: {np.sum(valid_mask)}/{nth_point_obp.size}\")\n",
    "    print(f\"Data completeness: {np.sum(valid_mask) / nth_point_obp.size * 100:.2f}%\")\n",
    "    \n",
    "    if np.any(valid_mask):\n",
    "        print(\n",
    "            f\"OBP range: {np.nanmin(nth_point_obp):.3f} \"\n",
    "            f\"to {np.nanmax(nth_point_obp):.3f} Pa\"\n",
    "        )\n",
    "        print(f\"Mean OBP: {np.nanmean(nth_point_obp):.3f} Pa\")\n",
    "        print(f\"OBP standard deviation: {np.nanstd(nth_point_obp):.3f} Pa\")\n",
    "    \n",
    "    return nth_point_obp\n",
    "\n",
    "\n",
    "def calculate_nth_point_mean_latitude(obp_ds, n=0):\n",
    "    \"\"\"\n",
    "    Calculate the mean latitude of the n-th point (point = n)\n",
    "    \n",
    "    Parameters:\n",
    "        obp_ds: xarray Dataset\n",
    "        n: point index (starting from 0)\n",
    "    \"\"\"\n",
    "    # Check index validity\n",
    "    if n >= obp_ds.dims[\"point\"]:\n",
    "        print(\n",
    "            f\"Error: index {n} is out of range, \"\n",
    "            f\"maximum index is {obp_ds.dims['point'] - 1}\"\n",
    "        )\n",
    "        return np.nan, None\n",
    "    \n",
    "    # Extract latitude at the n-th point\n",
    "    nth_point_lat = obp_ds.lat.isel(point=n).values  # shape: (depth,)\n",
    "    \n",
    "    print(f\"Latitude data for point {n}:\")\n",
    "    print(f\"Latitude array shape: {nth_point_lat.shape}\")\n",
    "    print(\n",
    "        f\"Latitude range: {np.nanmin(nth_point_lat):.3f}°N \"\n",
    "        f\"to {np.nanmax(nth_point_lat):.3f}°N\"\n",
    "    )\n",
    "    \n",
    "    # Compute mean latitude (ignore NaNs)\n",
    "    valid_lat_mask = ~np.isnan(nth_point_lat)\n",
    "    valid_lat_count = np.sum(valid_lat_mask)\n",
    "    \n",
    "    if valid_lat_count > 0:\n",
    "        mean_latitude = np.nanmean(nth_point_lat)\n",
    "        lat_std = np.nanstd(nth_point_lat)\n",
    "        lat_min = np.nanmin(nth_point_lat)\n",
    "        lat_max = np.nanmax(nth_point_lat)\n",
    "        \n",
    "        print(f\"\\nLatitude statistics for point {n}:\")\n",
    "        print(f\"  Mean latitude: {mean_latitude:.3f}°N\")\n",
    "        print(f\"  Latitude standard deviation: {lat_std:.3f}°\")\n",
    "        print(f\"  Latitude range: {lat_min:.3f}°N to {lat_max:.3f}°N\")\n",
    "        print(\n",
    "            f\"  Valid depth levels: {valid_lat_count}/{len(nth_point_lat)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Data completeness: \"\n",
    "            f\"{valid_lat_count / len(nth_point_lat) * 100:.2f}%\"\n",
    "        )\n",
    "        \n",
    "        # Show latitude values for the first 10 depth levels\n",
    "        print(\"\\nLatitude values for the first 10 depth levels:\")\n",
    "        for i in range(min(10, len(nth_point_lat))):\n",
    "            depth = obp_ds.depth.values[i]\n",
    "            lat = nth_point_lat[i]\n",
    "            if not np.isnan(lat):\n",
    "                print(f\"  Depth {depth:.0f} m: {lat:.3f}°N\")\n",
    "            else:\n",
    "                print(f\"  Depth {depth:.0f} m: NaN\")\n",
    "                \n",
    "    else:\n",
    "        print(f\"Warning: no valid latitude data for point {n}\")\n",
    "        mean_latitude = np.nan\n",
    "    \n",
    "    return mean_latitude, nth_point_lat\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
